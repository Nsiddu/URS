****************************************Mapjoin******************************
Map joins are really efficient if a table on the other side of a join is small enough to fit in the memory.
This is similar to DistributedCache in MapReduce framework.

set hive.auto.convert.join=true;
------------------------------------------------------------------------------
************************************Vectoriation******************************
------------------------------------------------------------------------------
Vectorization allows Hive to process a batch of rows together instead of processing one row at a time.
Vectorization improves the performance by fetching 1,024 rows in a single operation instead of fetching single 
row each time. It improves the performance for operations like filter, join, aggregation, etc.

set hive.vectorized.execution.enabled=true;
set hive.vectorized.execution.reduce.enabled=true;
---------------------------------------------------------------------------------
***********************************ORC filefromat********************************
---------------------------------------------------------------------------------

Optimized Row Columnar format provides highly efficient ways of storing the hive data by reducing the data storage format 
by 75% of the original. The ORCFile format is better than the Hive files format when it comes to reading, writing, and 
processing the data. It uses techniques like predicate push-down,compression, and more to improve the performance of the
query.

ORC supports compressed (ZLIB and Snappy), as well as uncompressed storage.
      
Create table orctbl (id int, name string, address string) stored as ORC tblproperties (“orc.compress”= “SNAPPY”);

----------------------------------------------------------------------------------------
**********************************Cost-Based Query Optimization*************************
---------------------------------------------------------------------------------------- 
set hive.cbo.enable=true;
set hive.compute.query.using.stats=true;
set hive.stats.fetch.column.stats=true;
set hive.stats.fetch.partition.stats=true;

------------------------------------------------------------------------------------------
***********************************Enable compression in Hive*****************************
------------------------------------------------------------------------------------------

Compression techniques reduce the amount of data being transferred and so reduces the data transfer between mappers and 
reducers.For better result, you need to perform compression at both mapper and reducer side separately.
Although gzip is considered as the best compression format but beware that it is not splittable and so should be applied
with caution.

Other formats are snappy, lzo, bzip, etc. You can set compression at mapper and reducer side using codes below-

set mapred.compress.map.output = true;
set mapred.output.compress= true;


******************************So let’s start with Hive performance tuning techniques!***********************

1. Use Tez to Fasten the execution

Apache TEZ is an execution engine used for faster query execution. 
It fastens the query execution time to around 1x-3x times.
To use TEZ execution engine, you need to enable it instead of default Map-Reduce execution engine.
TEZ can be enabled using the below query-

Set hive.execution.engine=tez;

-------------------------------------------------------------------------------------
2. Enable compression in Hive

Compression techniques reduce the amount of data being transferred and so reduces the data transfer between mappers and
reducers.For better result, you need to perform compression at both mapper and reducer side separately.
Although gzip is considered as the best compression format but beware that it is not splittable and so should be applied 
with caution.

Other formats are snappy, lzo, bzip, etc. You can set compression at mapper and reducer side using codes below-

set mapred.compress.map.output = true;
set mapred.output.compress= true;


---------------------------------------------------------------------------------------------------
3. Use ORC file format

ORC (optimized record columnar) is great when it comes to hive performance tuning. We can improve the query performance 
using ORC file format easily. You can check Hadoop file formats in detail here.There is no barrier like in which table you 
can use ORC file and in response, you get faster computation and compressed file size.

It is very easy to create ORC table, and you just need to add STORED AS ORC command as shown below.

Syntax:

Create table orctbl (id int, name string, address string) stored as ORC tblproperties (“orc.compress”= “SNAPPY”);
Now simply you can also insert the data like-

Insert overwrite table orctbl select * from tbldetails;


-----------------------------------------------------------------------------------------------------------
4. Optimize your joins

If you are using joins to fetch the results, it’s time to revise it. If you have large data in the tables, 
then it is not advisable to just use normal joins we use in SQL. There are many other joins like Map Join; bucket joins etc.
which can be used to improve Hive query performance.

You can do the following with joins to optimize hive queries-

4.1. Use Map Join
Map join is highly beneficial when one table is small so that it can fit into the memory. 
Hive has a property which can do auto-map join when enabled. Set the below parameter to true to enable auto map join.

Set hive.auto.convert.join to true to enable the auto map join. 
You can either set this from the command line or from the hive-site.xml file.

<property>
<name>hive.auto.convert.join</name>
<value>true</value>
<description>Whether Hive enables the optimization about converting common join into mapjoin based on the input file size</description>
</property>


4.2.Use Skew Join

Skew join is also helpful when your table is skewed. Set the hive.optimize.skewjoin property to true to enable skew join.

      set hive.optimize.skewjoin


<property>
<name>hive.optimize.skewjoin</name>
<value>true</value>
<description>
Whether to enable skew join optimization. The algorithm is as follows: At runtime, detect the keys with a large skew. 
Instead of processing those keys, store them temporarily in an HDFS directory. In a follow-up map-reduce job, process those
skewed keys. The same key need not be skewed for all the tables, and so, the follow-up map-reduce job (for the skewed keys)
would be much faster, since it would be a map-join.
</description> </property>


4.3Bucketed Map Join

If tables are bucketed by a particular column, you can use bucketed map join to improve the hive query performance.

You can set the below two property to enable the bucketed map join in Hive.

<property>
<name>hive.optimize.bucketmapjoin</name>
<value>true</value>
<description>Whether to try bucket mapjoin</description>
</property>
<property>
<name>hive.optimize.bucketmapjoin.sortedmerge</name>
<value>true</value>
<description>Whether to try sorted bucket merge map join</description>
</property>


--------------------------------------------------------------------------------------------------
5. Use partition

Partition is a useful concept in Hive. It is used to divide the large table based on certain column so that the whole data
can be divided into small chunks. It allows you to store the data under sub-directory inside a table.

Selecting the partition table is always a critical decision, and you need to take care of future data as well as the volume
of data as well. For example, if you have data of a particular location then partition based on state can be one of the 
ideal choices.

Here is the syntax to create partition table-

CREATE TABLE countrydata_partition
(Id int, countryname string, population int, description string)
PARTITIONED BY (country VARCHAR(64), state VARCHAR(64))
row format delimited
fields terminated by ‘\t’
stored AS textfile;
There are two types of partition in Hive-

Static partition
Dynamic partition
Static partition is the default one. To use dynamic partition in Hive, you need to set the following property-

set hive.exec.dynamic.partition=true;
set hive.exec.dynamic.partition.mode=nonstrict;
----------------------------------------------------------------------------------------------------------------
6. Bucketing can also be used

If you have more number of columns on which you want the partitions, bucketing in the hive can be a better option. 
We use CLUSTERED BY command to divide the tables in the bucket.

Here is the syntax to create bucketed table-

CREATE TABLE emp_bucketed_table(
ID int, name string, address string, salary string )
COMMENT ‘this is a bucketed table’
PARTITIONED BY (country VARCHAR(64))
CLUSTERED BY (state) INTO 10 BUCKETS
STORED AS TEXTFILE;
To enable bucketing in Hive, you need to set the following property-

SET hive.enforce.bucketing=true;
This should be set every time you are writing the data to the bucketed table.


----------------------------------------------------------------------------------------------
7. Parallel execution

As we know, Hive converts the queries into different stages during execution. These stages are usually getting executed one 
after the other and thus increases the time of execution. Below are some of the normal steps involved-

• MapReduce stage
• Sampling stage
• Limit stage
• Merge stage etc.

But the good thing is, you can set some of this independent stage to process parallel. This is a parallel execution in Hive. For this, you need to set the below properties to true-

Set hive.exec.parallel = true;

-------------------------------------------------------------------------------------------------

10. Avoid Global sorting

Global sorting in Hive is getting done by the help of the command ORDER BY in the hive. But the issue is, if you’re using
ORDER BY command, then the number of reducers will be set to one which can be illogical when you have large Hadoop dataset.

So when you don’t need global sorting, use SORT BY command which sorts the result per reducer.

Even you can also use DISTRIBUTE BY command if you want to control which particular rows will go with which reducer.

10.1 DISTRIBUTE BY

It ensures each of N reducers gets non-overlapping ranges of column, but doesn’t sort the output of each reducer. 
You end up with N or more unsorted files with non-overlapping ranges.

Example 

We are Distributing By x on the following 5 rows to 2 reducer:


x1
x2
x4
x3
x1
1
2
3
4
5
x1
x2
x4
x3
x1
Reducer 1


x1
x2
x1
1
2
3
x1
x2
x1
Reducer 2


x4
x3
1
2
x4
x3

Note that all rows with the same key x1 is guaranteed to be distributed to the same reducer (reducer 1 in this case),
 but they are not guaranteed to be clustered in adjacent positions.



10.2 CLUSTER BY

Cluster By is a short-cut for both Distribute By and Sort By.

CLUSTER BY x ensures each of N reducers gets non-overlapping ranges, then sorts by those ranges at the reducers.

Ordering : Global ordering between multiple reducers.

Outcome : N or more sorted files with non-overlapping ranges.

For the same example as above , if we use Cluster By x, the two reducers will further sort rows on x:

Reducer 1 :


x1
x1
x2
1
2
3
x1
x1
x2
Reducer 2 :

x3
x4
1
2
x3
x4

Instead of specifying Cluster By, the user can specify Distribute By and Sort By, so the partition columns and sort columns
 can be different.

10.3 Sort by:

Hive uses SORT BY to sort the rows based on the given columns per reducer. If there are more than one reducer, then the 
output per reducer will be sorted, but the order of total output is not guaranteed to be sorted.

set hive.mapred.reduce.tasks=2;

select * from emp sort by sal;

10.4 ORDER BY
Order by guarantees the total ordering of the output. Even if there are multiple reducers, the overall order of the output 
is maintained.

select * from emp orderby sal;



*******************************Compressions in hive *********************
SET hive.exec.compress.output=true;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET mapred.output.compression.type=BLOCK;

SET hive.exec.compress.output=true;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.GzipCodec;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.DefaultCodec;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.Bzip2Codec;
SET mapred.output.compression.codec=org.apache.hadoop.io.compress.lzo.LzopCodec;




10.5 File Formats:

HIVE STORAGE FORMATS

Among the different storage file formats that are used in hive, the default and simplest storage file format is the TEXTFILE.

10.5.1 TEXTFILE 

The data in a TEXTFILE is stored as plain text, one line per record. The TEXTFILE is very useful for sharing data with 
other tools and also when you want to manually edit the data in the file. However the TEXTFILE is less proficient when 
compared to the other formats.

SYNTAX :

CREATE TABLE TEXTFILE_TABLE (
COLUMN1 STRING,
COLUMN2 STRING,
COLUMN3 INT,
COLUMN4 INT
) STORED AS TEXTFILE;


10.5.2 SEQUENCE FILE
In sequence files the data is stored in a binary storage format consisting of binary key value pairs. A complete row is 
stored as single binary value. Sequence files are more compact than text and fit well the map-reduce output format. Sequence
files do support block compression and can be compressed on value, or block level, to improve its IO profile further.

SEQUENCEFILE is a standard format that is supported by Hadoop itself and is good choice for Hive table storage especially 
when you want to integrate Hive with other techonolgies in the Hadoop ecosystem.

The USING  sequence  file keywords lets you create a sequence  File. Here is an example statement to create a table using 
sequence File:

CREATE TABLE SEQUENCEFILE_TABLE (
COLUMN1 STRING,
COLUMN2 STRING,
COLUMN3 INT,
COLUMN4 INT
) STORED AS SEQUENCEFILE
Due to the complexity of reading sequence files, they are often only used for “in flight” data such as intermediate data 
storage used within a sequence of MapReduce jobs.


10.5.3 RCFILE OR RECORD COLUMNAR FILE

The RCFILE is one more file format that can be used with Hive. The RCFILE stores columns of a table in a record columnar 
format rather than row oriented fashion  and provides considerable compression and query performance benefits with highly 
efficient storage space utilization. Hive added the RCFile format in version 0.6.0.

RC file format is more useful when tables have large number of columns but only few columns are typically retrieved.
  
The RCFile combines multiple functions to provide the following features
Fast data storing
Improved query processing,
Optimized storage space utilization
Dynamic data access patterns.
SYNTAX:

CREATE TABLE RCFILE_TABLE (
COLUMN1 STRING,
COLUMN2 STRING,
COLUMN3 INT,
COLUMN4 INT ) STORED AS RCFILE;

Compressed RCFile reduces the IO and storage significantly over text, sequence file, and row formats. Compression on a
 column base is more efficient here since it can take advantage of similarity of the data in a column.


10.5.4 ORC FILE OR OPTIMIZED ROW COLUMNAR FILE

ORCFILE stands for Optimized Row Columnar File and it’s a new Hive File Format that was created to provide many advantages
 over the RCFILE format while processing data. The ORC File format comes with the Hive 0.11 version and cannot be used with
 previous versions.

Lightweight indexes are included with ORC file to improve the performance.
Also it uses specific encoders for different column data types to improve compression further, e.g. variable length 
compression on integers ORC stores collections of rows in one file and within the collection the row data is stored in a 
columnar format allowing parallel processing of row collections across a cluster.

ORC files compress better than RC files, enabling faster queries. To use it just add STORED AS orc to the end of your 
create table statements like this:

CREATE TABLE mytable (
COLUMN1 STRING,
COLUMN2 STRING,
COLUMN3 INT,
COLUMN4 INT
) STORED AS orc;

